{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GES-RNN-LAB-MELI-empty-students.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1kT_qNRKr9ayNt-goghwWRTLph1V1USZr",
      "authorship_tag": "ABX9TyM2RWmY4NsgaXBlIyrvufy7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorsergio/intro-to-deep-learning-2021/blob/main/Week_06_Lab_Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4ACF5klMzYC"
      },
      "source": [
        "## **Recuerde guardar una copia de este Colab notebook en su drive personal para no perder sus cambios.**\r\n",
        "\r\n",
        "### Recuerde tener el entorno de ejecución con aceleración GPU\r\n",
        "\r\n",
        "# Semana 06: NLP Natural Language Processing, Procesamiento de Lenguaje Natural.\r\n",
        "\r\n",
        "En este laboratorio realizará un clasificación de productos de una tienda virtual utilizando una Red LSTM creada con Tensorflow.\r\n",
        "\r\n",
        "Esta red LSTM sera entrenada con un conjunto de datos formado por nombres de productos en español subidos por usuarios al sitio de la tienda virtual.\r\n",
        "\r\n",
        "Cada nombre de producto corresponde a una categoría en la tienda virtual.\r\n",
        "\r\n",
        "Su modelo debera ser capaz de determinar la categoría correspondiente para un nombre de producto no conocido.\r\n",
        "\r\n",
        "\r\n",
        "Temas de aprendizaje objetivo:\r\n",
        "\r\n",
        "* Uso de la librería Pandas\r\n",
        "* Lectura de archivos .csv\r\n",
        "* Guardar un conjunto de datos en un archivo .csv\r\n",
        "* Limpieza de datos (texto)\r\n",
        "* Pre-procesamiento de texto (tokenización, padding, lematización, stopwords)\r\n",
        "* Uso  de la librería Stanza Stanford NLP\r\n",
        "* Uso de la librería NLTK\r\n",
        "* Codificación de etiquetas categóricas\r\n",
        "* Librería Seaborn (histogramas de frecuencias, matriz de confusión)\r\n",
        "* División de conjuntos de datos con estratificación y aleatorización (SKlearn)\r\n",
        "* Creación de word vectors con Fasttext\r\n",
        "* Transfer Learning en NLP (Word vectors, Embedding Layer)\r\n",
        "* Definición de modelos en Tensorflow con el API funcional\r\n",
        "* LSTM\r\n",
        "* BiDireccional LSTM\r\n",
        "* SparseCategorical Cross Entropy\r\n",
        "* Visualización de la arquitectura de un modelo (Plot model y summary)\r\n",
        "* Validation Split en el método Fit\r\n",
        "* Guardar y re-cargar un modelo de Tensorflow\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm5mYgr6cjos"
      },
      "source": [
        "# - Importe la librería tensorflow\r\n",
        "#   https://www.tensorflow.org/tutorials/quickstart/advanced\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF_OdluVcl97"
      },
      "source": [
        "# - Cargue la extensión Tensorboard para el notebook.\r\n",
        "# - Importe la librería datetime\r\n",
        "# - Cree el objeto log_dir \r\n",
        "# - Cree el objeto callback tensorboard_callback\r\n",
        "#   https://www.tensorflow.org/tensorboard/get_started\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "log_dir = \r\n",
        "tensorboard_callback = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB4rHBGmcngO"
      },
      "source": [
        "# - Ejecute el panel de TensorBoard para observar las gráficas de accuracy y loss.\r\n",
        "#   https://www.tensorflow.org/tensorboard/get_started\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qwFGombcpD3"
      },
      "source": [
        "# - Descargue el conjunto de datos con las descripciones de los productos de aquí: https://github.com/victorsergio/intro-to-deep-learning-2021/raw/main/meli_balanced_dataset_12.csv\r\n",
        "#   , utilizando el comando wget\r\n",
        "#   https://www.tutorialspoint.com/google_colab/google_colab_invoking_system_commands.htm\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMYtrkGzVQNe"
      },
      "source": [
        "# - Conceda permisos de acceso para que este Colab notebook pueda acceder a su\r\n",
        "#   Drive. En el menu de lado izquierdo, Archivos, Activar drive. \r\n",
        "# - Defina una ubicación en su Drive para almacenar archivos. La carpeta debe existir previamente,\r\n",
        "#   el path debe terminar con /\r\n",
        "# code:\r\n",
        "drive_path = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RQ_sHGWIqSn"
      },
      "source": [
        "# - Mueva el archivo .csv previamente descargado a la ubicación en su drive\r\n",
        "#   https://www.rapidtables.com/code/linux/mv.html\r\n",
        "#   el path debe terminar con /\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr0K6fyC0jCI"
      },
      "source": [
        "# - Importe la librería de Pandas.\r\n",
        "# - Lea el archivo csv.\r\n",
        "# - Muestre los primeros 5 elementos del dataframe.\r\n",
        "#   https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\r\n",
        "#   https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\r\n",
        "#   https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tFs09MF03an"
      },
      "source": [
        "# - Observe cuantas clases tiene su dataset.\r\n",
        "# - Observe cuantos datos tiene su dataset.\r\n",
        "#   https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwEtaA3gmcdD"
      },
      "source": [
        "# Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZUqsxY1mhAJ"
      },
      "source": [
        "# - Importe la librería unicodedata\r\n",
        "# - Importe la librería re\r\n",
        "# - Importe la librería string\r\n",
        "# - Cree una función llamada preprocess_text, que recibe como parámetro un \r\n",
        "#   string y devuelve: \r\n",
        "#       - El string con todos los caracteres en minúsculas, \r\n",
        "#       - sin caracteres especiales (tildes, diéresis, etc.). \r\n",
        "#       - sin caracteres de puntuación\r\n",
        "#       - sin espacios en blanco al inicio o al final.\r\n",
        "#   https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate\r\n",
        "#   https://www.w3schools.com/python/ref_string_strip.asp\r\n",
        "#   https://www.datacamp.com/community/tutorials/case-conversion-python\r\n",
        "\r\n",
        "\r\n",
        "# code imports:\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_text(text):\r\n",
        "\r\n",
        "    # code lowercase:\r\n",
        "    t = \r\n",
        "  \r\n",
        "    # remove accents\r\n",
        "    t = unicodedata.normalize('NFKD', t).encode('ASCII', 'ignore').decode('utf8')\r\n",
        "    \r\n",
        "    # code remove punctuation strings:\r\n",
        "    t = \r\n",
        "\r\n",
        "    # code remove start and end spaces:\r\n",
        "    t = \r\n",
        "\r\n",
        "    return t\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOjdz1VRpYj_"
      },
      "source": [
        "# - Aplique la función de preprocesamiento de texto sobre todas las filas\r\n",
        "#   de la columna 'title'\r\n",
        "#   https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMGhTeANp6vU"
      },
      "source": [
        "# - Muestre los primeros 5 elementos del dataframe\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS3WQ33XpNwB"
      },
      "source": [
        "# - Elimine todos las entradas que puedan estar vacias en el dataframe\r\n",
        "#   https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr_EyeqLL9qs"
      },
      "source": [
        "# - Observe la cantidad de entradas vacias en su dataset\r\n",
        "dataframe.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ouUnG5IqFHd"
      },
      "source": [
        "# Preprocesamiento del texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_P9pfvCpTol"
      },
      "source": [
        "# - Instale el paquete stanza de Stanford NLP\r\n",
        "#   https://pypi.org/project/stanza/\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEeIv13CqN3P"
      },
      "source": [
        "# - Importe la librería stanza\r\n",
        "# - Descargue el modelo para idioma español, paquete ancora, y los procesadores\r\n",
        "#   tokenize, mwt, pos, lemma\r\n",
        "# - Cree un objeto Pipeline, con el idioma español, paquete ancora y los procesadores\r\n",
        "#   tokenize, mwt, pos, lemma\r\n",
        "\r\n",
        "#   https://stanfordnlp.github.io/stanza/installation_usage.html#specifying-model-packages\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "\r\n",
        " \r\n",
        "nlp = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgW21728rYK5"
      },
      "source": [
        "# - Cree una función lemmatize_text, que recibe un string y devuelve el \r\n",
        "#   texto lematizado. Para esto aplique el pipeline anterior al string, y \r\n",
        "#   recorra el diccionario devuelto para formar un string con los lemas.\r\n",
        "\r\n",
        "#   https://stanfordnlp.github.io/stanza/pipeline.html#basic-example\r\n",
        "#   https://stanfordnlp.github.io/stanza/lemma.html#accessing-lemma-for-word\r\n",
        "\r\n",
        "\r\n",
        "def lemmatize_text(text):\r\n",
        "    \r\n",
        "    # code pipeline call:\r\n",
        "    doc =\r\n",
        "    t = \"\"\r\n",
        "    for sent in doc.sentences: \r\n",
        "        for word in sent.words:\r\n",
        "            # code lemma access:\r\n",
        "            t = t + #code:# + \" \"\r\n",
        "    \r\n",
        "    return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy5Cx_0Ord4G"
      },
      "source": [
        "# - Observe el funcionamiento de la función de lematización\r\n",
        "lemmatize_text(\"El joven se divierte corriendo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN9X7yryrne4"
      },
      "source": [
        "# - Aplique la función de lematizacion sobre todas las filas\r\n",
        "#   de la columna 'title'\r\n",
        "#   Tiempo de ejecución aproximado 17 minutos\r\n",
        "\r\n",
        "#   Para motivos de esta práctica no ejecute esta línea, por ser demasiado tardada.\r\n",
        "\r\n",
        "dataframe['title'] = dataframe.title.apply(lemmatize_text)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPiK9darp1ls"
      },
      "source": [
        "# - Aplique la función de preprocesamiento de texto preprocess_text nuevamente sobre todas las filas\r\n",
        "#   de la columna 'title', debido a que la lematización devuelve caracteres especiales.\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9WCgj1hsbkG"
      },
      "source": [
        "# - Importe la librería nltk\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Ke3T0lsmC7"
      },
      "source": [
        "# - Descargue los paquetes stopwords y punkt de la libreria nltk\r\n",
        "nltk.download()   #stopwords,punkt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFyL1P23suC6"
      },
      "source": [
        "# - Importe stopwords perteneciente a nltk.corpus\r\n",
        "#   https://pythonspot.com/nltk-stop-words/\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6kE0ADyOO3K"
      },
      "source": [
        "# - Observe los idiomas de los corpus de stopwords disponibles\r\n",
        "stopwords.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDgC8MBPsw0c"
      },
      "source": [
        "# - Complete la función que elimina stopwords a un string recibido\r\n",
        "#   esta función va creando un string solamente con las palabras que no \r\n",
        "#   se encuentran en el diccionario de stopwords en español.\r\n",
        "# - Guarde en stopWords el diccionario de stopwords del idioma español\r\n",
        "# - guarde en words la tokenizacion del string recibido\r\n",
        "# - Complete el ciclo con las variables correspondientes\r\n",
        "#   https://pythonspot.com/nltk-stop-words/\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "def remove_stopwords(text):\r\n",
        "    # code select spanish stopwords:\r\n",
        "    stopWords = \r\n",
        "\r\n",
        "    # code tokenization:\r\n",
        "    words = \r\n",
        "\r\n",
        "    wordsFiltered = \"\"\r\n",
        "    \r\n",
        "    \r\n",
        "    for w in words:\r\n",
        "        if w not in stopWords:\r\n",
        "            wordsFiltered = wordsFiltered + w + \" \"\r\n",
        "    \r\n",
        "    return wordsFiltered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlnL4NHHtAqS"
      },
      "source": [
        "# - Aplique la función remove_stopwords sobre todas las filas\r\n",
        "#   de la columna 'title'.\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7udgS45z2HmQ"
      },
      "source": [
        "# - Almacene el dataframe preprocesado resultante en un nuevo archivo llamado preprocessed.csv,\r\n",
        "#   recuerde guardarlo en su Drive, haciendo uso de la variable drive_path.\r\n",
        "# - No almacene los indices.\r\n",
        "#   https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU9N0o_gsapc"
      },
      "source": [
        "# - Ahora podra cargar nuevamente el dataframe preprocesado. Si por alguna\r\n",
        "#   razón este Colab se cierra puede iniciar desde esta línea cargando el \r\n",
        "#   dataframe preprocesado sin necesidad de ejecutar todo el preprocesamiento nuevamente.\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "dataframe = pd.read_csv(drive_path+'preprocessed.csv', sep=',')\r\n",
        "dataframe.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5_3uZrN05mr"
      },
      "source": [
        "# - Importe LabelEncoder y utilícelo para convertir sus etiquetas categóricas \r\n",
        "#   a índices numéricos. Almacene estos nuevos índices en una nueva columna. \r\n",
        "#   https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\r\n",
        "#   https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder.fit_transform\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "\r\n",
        "label_encoder = \r\n",
        "dataframe['label'] =  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug6yGr_k4ZBa"
      },
      "source": [
        "# - Elimine la columna category del dataframe\r\n",
        "#   https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw3xgKkHtlp3"
      },
      "source": [
        "# - Observe los primeros 5 ejemplos del dataframe.\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4onX6pN32G7h"
      },
      "source": [
        "# - Importe la librería seaborn como sns\r\n",
        "# - Muestre un gráfico de barras con la distribución de frecuencias de \r\n",
        "#   las clases.\r\n",
        "#   https://seaborn.pydata.org/introduction.html\r\n",
        "#   https://seaborn.pydata.org/generated/seaborn.countplot.html\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcsCC-D43XK4"
      },
      "source": [
        "# - Realice una division del conjunto de entrenamiento y prueba, aleatorizando la seleccion (random)\r\n",
        "#   con el 20% para el conjunto de prueba, y estratificando sobre las etiquetas.\r\n",
        "#   https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "# stratify by groups\r\n",
        "dataframe_train, dataframe_test = \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtCEcxYE7Cbx"
      },
      "source": [
        "# - Muestre la distribución de frecuencias de clases para el conjunto de entrenamiento.\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-sdJQzC7QEL"
      },
      "source": [
        "# - Muestre la distribución de frecuencias de clases para el conjunto de prueba\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmTQ0lPtJjAW"
      },
      "source": [
        "### Creación de los vectores de palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An2vPkc4JnXk"
      },
      "source": [
        "# - Instale el paquete fasttext\r\n",
        "#   https://pypi.org/project/fasttext/\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIKSLDAhJvf_"
      },
      "source": [
        "# - Guarde en su drive el archivo corpus.txt\r\n",
        "#   Este archivo debe contener la concateneción de cada una de las filas de la columna title del dataframe.\r\n",
        "#   https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.cat.html\r\n",
        "#   https://www.w3schools.com/python/ref_func_open.asp\r\n",
        "#   https://www.w3schools.com/python/python_file_write.asp\r\n",
        "\r\n",
        "# code create corpus txt file from procesed dataframe (concatenate strings)\r\n",
        "corpus = \r\n",
        "\r\n",
        "# code open new file corpus.txt in write mode\r\n",
        "corpus_file =\r\n",
        "# write the corpus to file\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoVLg_AqKnkF"
      },
      "source": [
        "# - Importe la librería fasttext\r\n",
        "# - Entrene un modelo no supervisado que cree los vectores para el corpus antes creado\r\n",
        "#    - dimensión de los vectores: 300\r\n",
        "#    - model: continuos bag of words\r\n",
        "# - tiempo aproximado de ejecución 2 minutos\r\n",
        "\r\n",
        "#   https://fasttext.cc/docs/en/unsupervised-tutorial.html\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "fasttext_model = \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0A_STbeK0Ex"
      },
      "source": [
        "# - Guarde el modelo de aprendizaje no supervisado que crea los vectores de palabras  en su drive.\r\n",
        "#   en un archivo llamado word_vectors.bin\r\n",
        "#   https://fasttext.cc/docs/en/python-module.html#saving-and-loading-a-model-object\r\n",
        "\r\n",
        "# code:\r\n",
        "fasttext_model."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08l7ONDXK5ap"
      },
      "source": [
        "#  - El archivo word_vectors.bin contiene el modelo creado y los vectores,\r\n",
        "#    solamente necesitamos los vectores en un archivo .vec.\r\n",
        "#    Esta parte del código crea un arhivo .vec en la cual en cada fila \r\n",
        "#    se encuentra la palabra y su vector numérico asociado.\r\n",
        "#    En resumen este código convierte un archivo .bin a un archivo .vec\r\n",
        "\r\n",
        "#  - Complete el código para obtener el archivo .vec word_vectors_300.vec\r\n",
        "\r\n",
        "#    https://fasttext.cc/docs/en/python-module.html#model-object\r\n",
        "\r\n",
        "\r\n",
        "# code open a new .vec file in write mode to export:\r\n",
        "file_out = \r\n",
        "\r\n",
        "# code  get the entire list of words of the dictionary:\r\n",
        "words = fasttext_model.\r\n",
        "\r\n",
        "# the first line must contain number of total words and vector dimension\r\n",
        "file_out.write(str(len(words)) + \" \" + str(fasttext_model.get_dimension()) + \"\\n\")\r\n",
        "\r\n",
        "for w in words:\r\n",
        "    # code get the vector representation of word:\r\n",
        "    v = fasttext_model.\r\n",
        "    vstr = \"\"\r\n",
        "    for vi in v:\r\n",
        "        vstr += \" \" + str(vi)\r\n",
        "    try:\r\n",
        "        file_out.write(w + vstr+'\\n')\r\n",
        "    except:\r\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvjWCeGZLCXe"
      },
      "source": [
        "### Uso de los vectores creados anteriormente en un modelo propio (Un tipo de transfer learning en NLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bltTwkcZxIIm"
      },
      "source": [
        "# - Cargue los vectores .vec creados anteriormente\r\n",
        "\r\n",
        "# code open the .vec file:\r\n",
        "file = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKZCpndh8weG"
      },
      "source": [
        "# - Analice detalladamente el siguiente código, que crea un diccionario vocab_and_vectors, con cada palabra\r\n",
        "#   como key, y su vector correspondiente como value.\r\n",
        "#   Almacena el contenido del archivo .vec en un diccionario.\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "vocab_and_vectors = {}\r\n",
        "# put words as dict indexes and vectors as words values\r\n",
        "for line in file:\r\n",
        "    values = line.split()\r\n",
        "    word = values[0] \r\n",
        "    vector = np.asarray(values[1:], dtype='float32')\r\n",
        "    vocab_and_vectors[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLooKboe8_Ew"
      },
      "source": [
        "# - Importe Tokenizer de keras.preprocessing.text \r\n",
        "# - Importe pad_sequences de keras.preprocessing.sequence\r\n",
        "# - Cree un objeto Tokenizer\r\n",
        "# - Obtenga un diccionario con Tokenize fit on texts para cada palabra del texto de la columna title\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# how many features should the tokenizer extract\r\n",
        "features = 1500\r\n",
        "\r\n",
        "# code tokenizer:\r\n",
        "tokenizer = \r\n",
        "\r\n",
        "# code fit the tokenizer on our text:\r\n",
        "tokenizer.\r\n",
        "\r\n",
        "# get all words that the tokenizer knows\r\n",
        "word_index = tokenizer.word_index\r\n",
        "\r\n",
        "print('Found %d unique words.' % len(tokenizer.word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpyaiiAsYjAv"
      },
      "source": [
        "# - Observe el diccionario creado por el Tokenizer\r\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGQTMKSG89qS"
      },
      "source": [
        "# - Establezca la longitud máxima de tokens en una oración (i.e. 20)\r\n",
        "# - 1. Convierta todos las filas de la columna title que estan en string a una representación\r\n",
        "#   numérica asignando los numéros correspondientes del diccionario creado por el tokenizer.\r\n",
        "# - 2. Recorte o aumente el tamaño de las oraciones para que todas tengan el máximo de max_length.\r\n",
        "# - 3. Almacene esas oraciones en X_train\r\n",
        "# - 4. Separe las etiquetas para que esten en y_train\r\n",
        "# - Realice los pasos 1,2,3 y 4 para X_test y y_test\r\n",
        "\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\r\n",
        "\r\n",
        "# code:\r\n",
        "max_length = \r\n",
        "\r\n",
        "# put the tokens in a matrix\r\n",
        "# code convert word sentences to numeric sentences:\r\n",
        "X_train = tokenizer.\r\n",
        "# code pad sentences:\r\n",
        "X_train =\r\n",
        "\r\n",
        "# code:\r\n",
        "y_train = \r\n",
        "\r\n",
        "# code convert word sentences to numeric sentences:\r\n",
        "X_test = tokenizer.\r\n",
        "# code pad sequences:\r\n",
        "X_test = \r\n",
        "# code\r\n",
        "y_test = \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW8Cp7xdit80"
      },
      "source": [
        "# - Observe un ejemplo del conjunto de entrenamiento\r\n",
        "X_train[120]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aNyMFm19sPl"
      },
      "source": [
        "# - Establezca la dimensión correcta del embedding, recuerde que esta\r\n",
        "#   dimensián debe ser la misma definida al crear los vectores\r\n",
        "# - Cree una matriz de 0s de dimensiones len(word_index) + 1 x la dimensón del embedding\r\n",
        "# - Observe el código que llena esta matriz que originalmente tiene 0s con los valores del diccionario vocab_and_vectors,\r\n",
        "#    diccionario que contiene los vectores numéricos obtenidos al entrenar el modelo no supervesiado de fasttext, vectores\r\n",
        "#    que estaban en el archivo .vec \r\n",
        "\r\n",
        "#   https://numpy.org/doc/stable/reference/generated/numpy.zeros.html\r\n",
        "\r\n",
        "# code:\r\n",
        "embedding_dim =   \r\n",
        "\r\n",
        "# code:\r\n",
        "embedding_matrix = \r\n",
        "\r\n",
        "for word, i in word_index.items():\r\n",
        "    embedding_vector = vocab_and_vectors.get(word)\r\n",
        "    # words that cannot be found will be set to 0\r\n",
        "    if embedding_vector is not None:\r\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTQetq99yWx"
      },
      "source": [
        "# - Establezca el numéro de clases para este problema de clasificación\r\n",
        "# - Defina el shape de entrada correspondiente para el tamaño de las secuencias.\r\n",
        "# - Cree un input_layer Embedding que contenga los pesos de la matriz de embedding.\r\n",
        "#    parámetros:\r\n",
        "#    - input_dim\r\n",
        "#    - output_dim\r\n",
        "#    - input_length\r\n",
        "#    - weights\r\n",
        "#    - trainable, Atención: Recuerde cual es el valor adecuado para esta variable,\r\n",
        "#                            debido a que estos valores ya estan pre-calculados\r\n",
        "\r\n",
        "# - Cree una capa LSTM bidireccional, que vaya seguida de input_layer\r\n",
        "#   - Parámetros:\r\n",
        "#     - units\r\n",
        "#     - return_sequences, Atención: recuerde el valor adecuado  para este parámetro\r\n",
        "\r\n",
        "# - Agregue una capa densa, con activación relu\r\n",
        "# - Cree la capa final para este problema de clasificación\r\n",
        "# - Cree un objeto Model, con las entradas y salidas correspondientes.\r\n",
        "#   https://www.tensorflow.org/guide/keras/functional\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/Input\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\r\n",
        "\r\n",
        "# code:\r\n",
        "num_classes = \r\n",
        "\r\n",
        "# code shape and dtype='int32', Input Layer\r\n",
        "inputs = t\r\n",
        "# code Embedding Layer:\r\n",
        "input_layer = \r\n",
        "# code:\r\n",
        "lstm1 =  \r\n",
        "# code:\r\n",
        "dense1 =\r\n",
        "# code:\r\n",
        "output = \r\n",
        "\r\n",
        "model = tf.keras.Model(inputs= ### , outputs= ### )\r\n",
        "\r\n",
        "# - Defina con el método compile, el optimizador Adam, learning rate, la función de pérdida a minimizar, \r\n",
        "#   el parámetro from_logits\r\n",
        "# - , y la métrica a mostrar en el entrenamiento\r\n",
        "# - Atencion a la correcta función de pérdida a optimizar, debido a la codificación de sus etiquetas.\r\n",
        "\r\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile\r\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\r\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "optimizer = \r\n",
        "loss = \r\n",
        "    \r\n",
        "model.compile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL2REnwe_AGq"
      },
      "source": [
        "# - Despliege un gráfico de la arquitectura de su modelo\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iiVQt8P_X_d"
      },
      "source": [
        "# - Muestre un sumario de los parámetros de su modelo, trainable parameters y \r\n",
        "#   non trainable parameters deben ser diferentes, sino es así hay un error en su definición de la arquitectura.\r\n",
        "# - Analice el por que deben ser diferentes para este caso.\r\n",
        "\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary\r\n",
        "\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrhFcYDIACPh"
      },
      "source": [
        "\r\n",
        "# - Cree un callback early stopping, para monitorear la pérdida de validación val_loss, para evitar el overfitting\r\n",
        "# - Parámetros:\r\n",
        "#   - monitor\r\n",
        "#   - patience\r\n",
        "#   - restore_best_weights\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\r\n",
        "\r\n",
        "# code:\r\n",
        "early_stopping_callback = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dYvZl5xW_cSU"
      },
      "source": [
        "# - Entrene el modelo con el método fit\r\n",
        "#   Especifique el generador de entrenamiento, \r\n",
        "#   la división para conjunto de validación (i.e. 20%),\r\n",
        "#   El número de épocas a entrenar\r\n",
        "#   el callback de tensorboard\r\n",
        "#   el callback de early stoppinng\r\n",
        "#   el tamaño de batch\r\n",
        "\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "history = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwJHQMhVE5td"
      },
      "source": [
        "# - Evalúe los resultados con el método evaluate, sobre los datos de prueba\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\r\n",
        "# code:\r\n",
        "\r\n",
        "# Evaluate the model on test data using `evaluate`\r\n",
        "print('\\n# Evaluate on test data')\r\n",
        "results = \r\n",
        "print('test loss, test acc:', results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcsqNhFKH9iJ"
      },
      "source": [
        "# - Guarde su modelo en una carpeta en su drive\r\n",
        "#   https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model\r\n",
        "# code:\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-VDZKc9IA5N"
      },
      "source": [
        "# - Cargue nuevamente el modelo ya entrenado desde su drive\r\n",
        "#   https://www.tensorflow.org/tutorials/keras/save_and_load#savedmodel_format\r\n",
        "\r\n",
        "# code:\r\n",
        "model = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PopWiw53FCbR"
      },
      "source": [
        "# - Obtenga las predicciones entregadas por su modelo con el método predict sobre los datos  de prueba\r\n",
        "#   https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "probs = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhFz_yVpF7Hn"
      },
      "source": [
        "# - Debe obtener la misma cantidad de predicciones que la cantidad de elementos en su conjunto de prueba,\r\n",
        "#   debe obtener una probabilidad por cada clase, para cada ejemplo en el conjunto de prueba\r\n",
        "\r\n",
        "print(probs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voR9a8V0GCbv"
      },
      "source": [
        "# - Con argmax podemos obtener la posición del elemento en el vector de probabilidades que tenga el valor mayor.\r\n",
        "#   https://numpy.org/doc/stable/reference/generated/numpy.argmax.html\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "predicted_labels = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc6t-9_eGK2w"
      },
      "source": [
        "# - Observe las dimensiones obtenidas al aplicar np.argmax sobre las predicciones\r\n",
        "\r\n",
        "predicted_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoMg9qauGOBs"
      },
      "source": [
        "# - Importe confusion_matrix del paquete sklearn y cree la matriz de confusión, esta matriz se visualizará de una forma numérica simple.\r\n",
        "#   https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\r\n",
        "\r\n",
        "# code:\r\n",
        "\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "# Calculate the confusion matrix.\r\n",
        "cm = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azfNXJghGbty"
      },
      "source": [
        "# Aquí se desplegara la matriz de confusión en forma numérica\r\n",
        "\r\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q57mYkPtGb-Z"
      },
      "source": [
        "# - Con la librería Seaborn se muestra la matriz de confusión con una mejor visualizacion gráfica, esta matriz de confusión esta normalizada\r\n",
        "#   Observe y analice que parámetros recibe sns.heatmap()\r\n",
        "\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "fig, ax = plt.subplots(figsize=(10,10))\r\n",
        "sns.heatmap(cm/cm.astype(np.float).sum(axis=1), annot=True, fmt='.2%',ax=ax)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dX-PxyVhUQ3"
      },
      "source": [
        "# - Realice una prueba para un texto simple\r\n",
        "# - Establezca el texto\r\n",
        "#   https://numpy.org/doc/stable/reference/generated/numpy.array.html\r\n",
        "\r\n",
        "# code the test text must be a string inside a numpy array:\r\n",
        "test = \r\n",
        "\r\n",
        "# - Convierta el texto de prueba a secuencias\r\n",
        "# - Realize el padding del texto de prueba\r\n",
        "# - Realice la prediccion sobre la oración de prueba\r\n",
        "# - aplique np.argmax sobre el vector de probabilidades\r\n",
        "# - obtenga los resultados\r\n",
        "\r\n",
        "tokenized_test =  \r\n",
        "padded_test = \r\n",
        "\r\n",
        "# code predict:\r\n",
        "probs_test = \r\n",
        "# code argmax:\r\n",
        "predicted_labels_test = \r\n",
        "\r\n",
        "print(predicted_labels_test)\r\n",
        "\r\n",
        "#  - Convierta el valor numérico de su etiqueta a una etiqueta categorica para \r\n",
        "#    una mejor lectura, para esto debe utilizar el objeto label encoder que utilizó\r\n",
        "#    al principio para convertir etiquetas categóricas a numíricas, deberá realizar el proceso inverso\r\n",
        "#    https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder.inverse_transform\r\n",
        "\r\n",
        "categorical_prediction =  \r\n",
        "print(categorical_prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikM19C-0kVhm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}